{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7044d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import copy\n",
    "\n",
    "# py.init_notebook_mode()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a65b2bc",
   "metadata": {},
   "source": [
    "## Notations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f92ed7",
   "metadata": {},
   "source": [
    "N: the number of nodes\n",
    "\n",
    "d: dimension of the feature vectors\n",
    "\n",
    "i: node index\n",
    "\n",
    "k: iteration index\n",
    "\n",
    "$\\gamma$: learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb42f39",
   "metadata": {},
   "source": [
    "## Linear regression problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6b73fe",
   "metadata": {},
   "source": [
    "We consider the following linear regression problem:\n",
    "\n",
    "$\\min_{x \\in \\mathbb{R}^d} f(x) := \\frac{1}{N}\\sum_{i=1}^N f_i(x) \\quad$  where  $\\quad f_i(x) = \\frac{1}{2}\\|A_i x - b_i\\|^2$\n",
    "\n",
    "In the above problem, $N$ is the number of nodes, each local cost function $f_i(x)$ is kept by node $i$. Each node can only access its own local cost function $f_i(x)$ and gradient $\\nabla f_i(x)$. Each local $A_i \\in \\mathbb{R}^{m \\times d}$  and $b_i \\in \\mathbb{R}^m$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57034beb",
   "metadata": {},
   "source": [
    "#### Generate synthesized data for linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a762c9",
   "metadata": {},
   "source": [
    "In math, we let $A = [A_1; A_2; \\cdots; A_n] \\in \\mathbb{R}^{Nm\\times d}$ and $b = [b_1;b_2;\\cdots;b_n] \\in \\mathbb{R}^{Nm}$. The linear regression problem can be rewritten as\n",
    "\n",
    "$\\min_{x \\in \\mathbb{R}^d} f(x) := \\frac{1}{2N}\\|A x - b\\|^2.$\n",
    "\n",
    "The optimal solution can be derived as $x^\\star = (A^\\top A)^{-1}(A^\\top b)$\n",
    "\n",
    "When we generate data, we directly generate the entire $A$ and $b$. Each local data $A_i$ and $b_i$ can be obtained by retrieving the corresponding block in $A$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ffcb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_generate_data(total_sample_size, d, x0, noise_scale):\n",
    "    \n",
    "    '''\n",
    "    This function is to generate data (A, b) via b = A*x0 + noise\n",
    "    Input:\n",
    "        total_sample_size: size of all samples, i.e., N*m\n",
    "                        d: variable dimension \n",
    "                       x0: an auxilliary variable with dimension d that helps generate b\n",
    "              noise_scale: controls the scale of noise\n",
    "    '''\n",
    "    \n",
    "    A = np.random.randn(total_sample_size, d) \n",
    "    b = A@x0 + noise_scale * np.random.randn(total_sample_size, 1)\n",
    "    \n",
    "    return A, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c178434b",
   "metadata": {},
   "source": [
    "#### Calculate the optimal solution to the linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99aef4c",
   "metadata": {},
   "source": [
    "The optimal solution can be derived as $x^\\star = (A^\\top A)^{-1}(A^\\top b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49549060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_sol(A, b):\n",
    "    \n",
    "    return np.linalg.inv(A.T@A)@(A.T@b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2961f494",
   "metadata": {},
   "source": [
    "#### Test utilities to generate data and get solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b824b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, m, d = 10, 5, 5\n",
    "total_sample_size = N * m\n",
    "x0 = np.random.randn(d, 1)\n",
    "noise_scale = 0.1\n",
    "\n",
    "# Generate data\n",
    "A, b   = lr_generate_data(total_sample_size, d, x0, noise_scale)\n",
    "\n",
    "# Get optimal solution\n",
    "x_star = get_lr_sol(A, b)\n",
    "\n",
    "# Test the optimality of x_star\n",
    "grad = A.T@(A@x_star - b)\n",
    "if(np.linalg.norm(grad) <= 1e-10):\n",
    "    print(\"x_star is the optimal solution\")\n",
    "else:\n",
    "    print(\"x_star is not the optimal solution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d97860",
   "metadata": {},
   "source": [
    "## FedAvg algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1c569c",
   "metadata": {},
   "source": [
    "#### A standard algorithmic recursion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83a5913",
   "metadata": {},
   "source": [
    "Recall the following linear regression problem:\n",
    "\n",
    "$\\min_{x \\in \\mathbb{R}^d} f(x) := \\frac{1}{N}\\sum_{i=1}^N f_i(x) \\quad$  where  $\\quad f_i(x) = \\frac{1}{2}\\|A_i x - b_i\\|^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc93507",
   "metadata": {},
   "source": [
    "The FedAvg algorithm is as follows:\n",
    "\n",
    "$\\psi_i^{(k)} = x_i^{(k)} - \\gamma \\nabla f(x_i^{(k)})$ for all node $i$ in parallel\n",
    "\n",
    "$x_i^{(k+1)} = (1/n)\\sum_{i=1}^n \\psi_i^{(k)}$ if $\\mathrm{mod}(k, \\tau) = 0$ otherwise $x_i^{(k+1)} = \\psi_i^{(k)}$\n",
    "\n",
    "where $\\tau$ is the number of local updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e73029",
   "metadata": {},
   "source": [
    "#### Rewrite the algorithm in a more compact manner\n",
    "\n",
    "Generally speaking, simulating the algorithm in a real distributed manner is resource consuming, and here we only validate the convergence performance of the FedAvg algorithm rather than its wall-clock performance. To this end, we introduce\n",
    "\n",
    "$X = [x_1^\\top; x_2^\\top; \\cdots; x_N^\\top] \\in \\mathbb{R}^{N \\times d}$\n",
    "\n",
    "$\\Psi = [\\psi_1^\\top; \\psi_2^\\top; \\cdots; \\psi_N^\\top] \\in \\mathbb{R}^{N \\times d}$\n",
    "\n",
    "$G = [(\\nabla f_1(x_1))^\\top; (\\nabla f_2(x_2))^\\top; \\cdots; (\\nabla f_N(x_N))^\\top] \\in \\mathbb{R}^{N \\times d}$\n",
    "\n",
    "With the above notations, we can rewrite the FedAvg algorithm as follows\n",
    "\n",
    "$X^{(k+1)} = W\\left( X^{(k)} - \\gamma G^{(k)} \\right)$\n",
    "\n",
    "where\n",
    "\n",
    "$ W = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "I & \\mbox{if $\\mathrm{mod}(k,\\tau) \\neq 0$} \\\\\n",
    "(1/n)\\mathbf{1}_n \\mathbf{1}_n^\\top & \\mbox{if $\\mathrm{mod}(k,\\tau) = 0$}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c8dc6a",
   "metadata": {},
   "source": [
    "#### Implementing the FedAvg algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd65bb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_G(A, b, X, N, d):\n",
    "    \n",
    "    '''\n",
    "    This function is to get the gradient matrix G which is defined above\n",
    "    X : the variable matrix X which is defined above\n",
    "    '''\n",
    "    \n",
    "    total_sample_size, _ = A.shape\n",
    "    local_sample_size    = total_sample_size // N\n",
    "    \n",
    "    G = np.zeros((N, d))\n",
    "    for i in range(N):\n",
    "        \n",
    "        Ai = A[i*local_sample_size:(i+1)*local_sample_size,:]\n",
    "        bi = b[i*local_sample_size:(i+1)*local_sample_size,:]\n",
    "        xi = X[i,:].reshape(-1,1)\n",
    "        grad = Ai.T@(Ai@xi - bi)\n",
    "        G[i,:] = grad.reshape(1,-1)\n",
    "        \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b5723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FedAvg(A, b, x_star, tau, N, d, gamma, num_iters):\n",
    "    \n",
    "    '''\n",
    "    This function is to implement the FedAvg algorithm\n",
    "    x_star    : the optimal solutioin. We use it as the reference solution\n",
    "    tau       : the local update period\n",
    "    gamma     : learning rate\n",
    "    num_iters : the number of FedAvg iterations\n",
    "    '''\n",
    "    \n",
    "    X = np.zeros((N, d))\n",
    "    error = np.zeros(num_iters)\n",
    "    \n",
    "    for ite in range(num_iters):\n",
    "        \n",
    "        # decide whether to conduct local update or to global averaging\n",
    "        if ite%tau == 0:\n",
    "            W = np.ones((N, N))/N\n",
    "        else:\n",
    "            W = np.eye(N)\n",
    "            \n",
    "        # FedAvg main recursion\n",
    "        G   = get_G(A, b, X, N, d)\n",
    "        Phi = X - gamma * G\n",
    "        X   = W@Phi\n",
    "        \n",
    "        # record the error\n",
    "        error[ite] = np.linalg.norm(X - x_star.reshape(1,-1))/N\n",
    "        \n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8536e44c",
   "metadata": {},
   "source": [
    "## FedAvg cannot converge to the optimal solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5501247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, m, d = 10, 5, 5\n",
    "total_sample_size = N * m\n",
    "x0 = np.random.randn(d, 1)\n",
    "noise_scale = 0.1\n",
    "\n",
    "# Generate data\n",
    "A, b   = lr_generate_data(total_sample_size, d, x0, noise_scale)\n",
    "\n",
    "# Get optimal solution\n",
    "x_star = get_lr_sol(A, b)\n",
    "\n",
    "# Test the optimality of x_star\n",
    "grad = A.T@(A@x_star - b)\n",
    "if(np.linalg.norm(grad) <= 1e-10):\n",
    "    print(\"x_star is the optimal solution\")\n",
    "else:\n",
    "    print(\"x_star is not the optimal solution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b19e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = FedAvg(A, b, x_star = x_star, tau = 10, N = N, d = d, gamma = 0.01, num_iters = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd3978",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(error, linewidth = 2.0)\n",
    "plt.xlabel('iteartion', fontsize = 16)\n",
    "plt.ylabel('distance to optimal solution', fontsize = 16)\n",
    "plt.legend(['FedAvg'], fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53934b3f",
   "metadata": {},
   "source": [
    "It is observed that FedAvg oscillates and cannot converge to the optimal solution.\n",
    "\n",
    "It is because FedAvg suffers from data heterogeneity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9266353d",
   "metadata": {},
   "source": [
    "## My cool algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa4020f",
   "metadata": {},
   "source": [
    "Recall the FedAvg recursion: \n",
    "    \n",
    "$X^{(k+1)} = W\\left( X^{(k)} - \\gamma G^{(k)} \\right)$\n",
    "\n",
    "It is very similar to the decentralized gradient descent (DGD) algorithm but with a time-varying mixing matrix\n",
    "\n",
    "$ W = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "I & \\mbox{if $\\mathrm{mod}(k,\\tau) \\neq 0$} \\\\\n",
    "(1/N)\\mathbf{1}_N \\mathbf{1}_N^\\top & \\mbox{if $\\mathrm{mod}(k,\\tau) = 0$}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$\n",
    "\n",
    "This inspires us to correct the data heterogeneity suffered in FedAvg by utilizing the techniques in decentralized optimization. Check the corresponding lecture slides for decentralized optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d009b83",
   "metadata": {},
   "source": [
    "### My cool algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f9d6cd",
   "metadata": {},
   "source": [
    "Please write down the math recursions of your cool algorithm below:\n",
    "\n",
    "[Write down the algorithm recursion here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038ac609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please implement your cool algorithm here\n",
    "def MyCoolAlg(Your input argument):\n",
    "    \n",
    "    '''\n",
    "    This function is to implement your cool algorithm \n",
    "    Please describe your input argument here\n",
    "    '''\n",
    "    \n",
    "    error = np.zeros(num_iters)\n",
    "    \n",
    "    # Implement your algorihtm here\n",
    "        \n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122f3dc6",
   "metadata": {},
   "source": [
    "### Compare my cool algorithm with FedAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a933fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, m, d = 10, 5, 5\n",
    "total_sample_size = N * m\n",
    "x0 = np.random.randn(d, 1)\n",
    "noise_scale = 0.1\n",
    "\n",
    "# Generate data\n",
    "A, b   = lr_generate_data(total_sample_size, d, x0, noise_scale)\n",
    "\n",
    "# Get optimal solution\n",
    "x_star = get_lr_sol(A, b)\n",
    "\n",
    "# Test the optimality of x_star\n",
    "grad = A.T@(A@x_star - b)\n",
    "if(np.linalg.norm(grad) <= 1e-10):\n",
    "    print(\"x_star is the optimal solution\")\n",
    "else:\n",
    "    print(\"x_star is not the optimal solution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a32b051",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 10\n",
    "gamma = 0.01\n",
    "num_iters = 1000\n",
    "\n",
    "error_fedavg  = FedAvg(A, b, x_star = x_star, tau = tau, N = N, d = d, gamma = gamma, num_iters = num_iters)\n",
    "error_coolalg = MyCoolAlg(Your input argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887987ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(np.arange(num_iters), error_fedavg, 'b-', np.arange(num_iters), error_coolalg, 'r-', linewidth = 2.0)\n",
    "plt.xlabel('iteartion', fontsize = 16)\n",
    "plt.ylabel('distance to optimal solution', fontsize = 16)\n",
    "plt.legend(['FedAvg', 'MyCoolAlg'], fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07572a0",
   "metadata": {},
   "source": [
    "You should be able to observe that your new algorithm will converge with accuracy better than 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbb37e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if error_coolalg[-1] < 1e-10:\n",
    "    print(\"Congratulations! Your cool algorithm has corrected the data heterogeneity suffered by FedAvg! If you can further provide convergence analyses, you may wrap them into a paper and submit it to a good conference or jounral :)\")\n",
    "else:\n",
    "    print(\"Please try again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7ab924",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
