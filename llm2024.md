---
title: ""
permalink: "/llm2024/"
layout: page
---

## PKU Class 2024 Spring: Large Language Model in Decision Intelligence

Instructor: **Kun Yuan** (kunyuan@pku.edu.cn) <br>

Teaching assistants: 
- **Yudong Bai** (yutonghe@pku.edu.cn) <br>
- **Yunteng Geng** (2301213081@pku.edu.cn) <br>
- **Yutong He** (yutonghe@pku.edu.cn) <br>
- **Peijin Li** (2301213056@stu.pku.edu.cn) <br>
- **Zihao Liu** (2100011704@stu.pku.edu.cn) <br>
- **Keer Lu** (2301213094@stu.pku.edu.cn) <br>
- **Yilong Song** (2301213059@pku.edu.cn) <br> 
- **Qianyou Sun** (2301111049@stu.pku.edu.cn) <br>
- **Yuchi Wang** (wangyuchi@stu.pku.edu.cn) <br>

Office hour: 4pm - 5pm Wednesday, 静园六院220

## References
Stanford CS224n: Natural Language Processing with Deep Learning

## Lectrures

### Lecture 1: Introduction to LLM <br>
- Introduction to large language model [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/Intro_LLM_v1.pdf)
- Reading: <br>
    - Andrej Karpathy, *[State of GPT](https://www.bilibili.com/video/BV1ts4y1T7UH/?spm_id_from=333.337.search-card.all.click)* <br>
    - Andrej Karpathy, *[The busy person's intro to LLM](https://www.bilibili.com/video/BV1NH4y1m78m/?spm_id_from=333.337.search-card.all.click&vd_source=2609112b8838130df3f5c7166ed6effb)* <br>

### Lecture 2: Linear algebra and optimization <br>
- Convex set; Convex functions; Gradient descent; Convergence [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/gradient_descent.pdf) <br>
- Notes: [[Linear algebra]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/notes_linear_algebra.pdf) [[Gradient descent]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/notes_gradient_descent.pdf)

### Lecture 3: Basics in machine learning <br>
- Linear regression; Logistic regression; Multi-classification; Neural network [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/ml.pdf)
- Reading: <br>
    - Stanford CS231n, *[Linear classification](https://cs231n.github.io/linear-classify/)* <br>
    - Stanford CS231n, *[Neural netowrk part I](https://cs231n.github.io/neural-networks-1/)* 

### Lecture 4: Word embedding and language models <br>
- Word embedding; [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/langmodel.pdf)
- Language models; Recurrent neural network; (Slides are adapted from Stanford [CS224n RNN](https://web.stanford.edu/class/cs224n/slides/cs224n-2024-lecture05-rnnlm.pdf))
- Back propogation in RNN [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/RNN_grad.pdf)
- Sequence-to-sequence model (Slides are adapted from Stanford [CS224n Seq2Seq](https://web.stanford.edu/class/cs224n/slides/cs224n-2024-lecture06-fancy-rnn.pdf))

### Lecture 5: Transformer <br>
- Forward-Backward propogation [Hand-written materials]
- Transformers (Slides are adapted from Stanford [CS224n Transformers](https://web.stanford.edu/class/cs224n/slides/cs224n-2024-lecture08-transformers.pdf))
- Parameters and Computations in Transformers [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/Memory_analysis.pdf)
- Reading: <br>
    - Illustrated Guide to Transformers Neural Network: A step by step explanation. [[Youtube video]](https://www.youtube.com/watch?v=4Bdc55j80l8) [[Bilibili video]](https://www.bilibili.com/video/BV1AK4y1e7y1/?vd_source=2609112b8838130df3f5c7166ed6effb)
 
### Guest Lecture I: <br> ###
- Large language model in mathematical reasoning (Dr. Jihai Zhang, Alibaba DAMO Academcy) 

### Lecture 6: Pretrain and Fine-tune Paradigm <br>
- Teacher forcing; Pretrain; Fine-tune; BERT; GPTs [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/Pre_train.pdf)
- Reading: <br>
    - J. Devlin et.al., *[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)*
    - A. Radford et.al., *[Improving Language Understanding by Generative Pre-Training](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)*
    - A. Radford et.al., *[Language models are unsupervised multitask learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)*
    - T. B. Brown et.al., *[Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)*

### Lecture 7: Optimizers <br>
- Stochastic gradient descent [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/SGD.pdf) [[Notes]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/SGD_convergence.pdf)
- Momentum SGD [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/ACC_SGD.pdf)
- Adagrad; RMSProp; Adam [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/Adaptive_SGD.pdf)
- Memories in Transformer [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/Memory_analysis_part2.pdf)
- Mixed-precision training [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/mixed_precision.pdf)

### Midterm Exam
- Midterm review [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/Midterm_review.pdf)

### Lecture 8: Distributed Training <br>
- Scaling law [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/Scaling_law.pdf)
- Data parallelism; distributed SGD
- Pipeline parallelism
- Tensor parallelism
- Zero-redundancy optimizer

### Lecture 9: Data Collection and Cleaning
- Data collection
- Data cleaning


