---
title: ""
permalink: "/llm2024/"
layout: page
---

## PKU Class 2024 Spring: Introduction to Large Language Models

Instructor: **Kun Yuan** (kunyuan@pku.edu.cn) <br>

Sponsor: **Decision Intelligence Team, Alibaba DAMO Academy** <br>

Teaching assistants: 
- **Yudong Bai** (yutonghe@pku.edu.cn) <br>
- **Yunteng Geng** (2301213081@pku.edu.cn) <br>
- **Yutong He** (yutonghe@pku.edu.cn) <br>
- **Peijin Li** (2301213056@stu.pku.edu.cn) <br>
- **Zihao Liu** (2100011704@stu.pku.edu.cn) <br>
- **Keer Lu** (2301213094@stu.pku.edu.cn) <br>
- **Yilong Song** (2301213059@pku.edu.cn) <br> 
- **Qianyou Sun** (2301111049@stu.pku.edu.cn) <br>
- **Yuchi Wang** (wangyuchi@stu.pku.edu.cn) <br>

Office hour: 4pm - 5pm Wednesday, 静园六院220

## References
Stanford CS224n: Natural Language Processing with Deep Learning

## Lectrures

### Lecture 1: Introduction to LLM <br>
- Introduction to large language model [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/Intro_LLM_v1.pdf)
- Reading: <br>
    - Andrej Karpathy, *[State of GPT](https://www.bilibili.com/video/BV1ts4y1T7UH/?spm_id_from=333.337.search-card.all.click)* <br>
    - Andrej Karpathy, *[The busy person's intro to LLM](https://www.bilibili.com/video/BV1NH4y1m78m/?spm_id_from=333.337.search-card.all.click&vd_source=2609112b8838130df3f5c7166ed6effb)* <br>

### Lecture 2: Linear algebra and optimization <br>
- Convex set; Convex functions; Gradient descent; Convergence [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/gradient_descent.pdf) <br>
- Notes: [[Linear algebra]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/notes_linear_algebra.pdf) [[Gradient descent]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/notes_gradient_descent.pdf)

### Lecture 3: Basics in machine learning <br>
- Linear regression; Logistic regression; Multi-classification; Neural network [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/ml.pdf)
- Reading: <br>
    - Stanford CS231n, *[Linear classification](https://cs231n.github.io/linear-classify/)* <br>
    - Stanford CS231n, *[Neural netowrk part I](https://cs231n.github.io/neural-networks-1/)* 

### Lecture 4: Word embedding and language models <br>
- Word embedding; [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/langmodel.pdf)
- Language models; Recurrent neural network; (Slides are adapted from Stanford [CS224n RNN](https://web.stanford.edu/class/cs224n/slides/cs224n-2024-lecture05-rnnlm.pdf))
- Back propogation in RNN [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/RNN_grad.pdf)
- Sequence-to-sequence model (Slides are adapted from Stanford [CS224n Seq2Seq](https://web.stanford.edu/class/cs224n/slides/cs224n-2024-lecture06-fancy-rnn.pdf))

### Lecture 5: Transformer <br>
- Forward-Backward propogation [Hand-written materials]
- Transformers (Slides are adapted from Stanford [CS224n Transformers](https://web.stanford.edu/class/cs224n/slides/cs224n-2024-lecture08-transformers.pdf))
- Parameters and Computations in Transformers [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/Memory_analysis.pdf)
- Reading: <br>
    - Illustrated Guide to Transformers Neural Network: A step by step explanation. [[Youtube video]](https://www.youtube.com/watch?v=4Bdc55j80l8) [[Bilibili video]](https://www.bilibili.com/video/BV1AK4y1e7y1/?vd_source=2609112b8838130df3f5c7166ed6effb)
 
### Guest Lecture I: <br> ###
- Large language model in mathematical reasoning (Dr. Jihai Zhang, Alibaba DAMO Academcy) 

### Lecture 6: Pretrain and Fine-tune Paradigm <br>
- Teacher forcing; Pretrain; Fine-tune; BERT; GPTs [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/Pre_train.pdf)
- Reading: <br>
    - J. Devlin et.al., *[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)*
    - A. Radford et.al., *[Improving Language Understanding by Generative Pre-Training](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)*
    - A. Radford et.al., *[Language models are unsupervised multitask learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)*
    - T. B. Brown et.al., *[Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)*

### Lecture 7: Optimizers <br>
- Stochastic gradient descent [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/SGD.pdf) [[Notes]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/SGD_convergence.pdf)
- Momentum SGD [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/ACC_SGD.pdf)
- Adagrad; RMSProp; Adam [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/Adaptive_SGD.pdf)
- Memories in Transformer [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/Memory_analysis_part2.pdf)
- Mixed-precision training [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/mixed_precision.pdf)

### Midterm Exam
- Midterm review [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/Midterm_review.pdf)

### Lecture 8: Distributed Training <br>
- Scaling law [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/Scaling_law.pdf)
- Data parallelism and communication saving; Pipeline parallelism; Tensor parallelism [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/DistributedTraining.pdf)
- Reading: <br>
    - Jared Kaplan, et. al. *[Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)*
    - Jordan Hoffmann, et. al. *[Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)*
    - Tal Ben-Nun and Torsten Hoefler, *[Demystifying Parallel and Distributed Deep Learning: An In-Depth Concurrency Analysis](https://arxiv.org/abs/1802.09941)*
    - Kun Yuan *[Distributed Machine Learning: Part I](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/resources/talk/DistributedML-PartI[Okinawa].pdf)*
    - Kun Yuan *[Distributed Machine Learning: Part II](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/resources/talk/DistributedML-Part2[Okinawa].pdf)*

### Lecture 9: Data Prepation
- Data source; Deduplication; Quality filtering; Sensitive information reduction; Data composition; Data curriculum [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/DataPrep.pdf)
- Reading: <br>
    - Penedo, Guilherme, et al., *[The Refined Web Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only](https://arxiv.org/pdf/2306.01116)*
    - Soldaini, Luca, et al., *[Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research](https://arxiv.org/pdf/2402.00159)*
    - Kandpal, Nikhil, Eric Wallace, and Colin Raffel., *[Deduplicating Training Data Mitigates Privacy Risks in Language Models](https://proceedings.mlr.press/v162/kandpal22a/kandpal22a.pdf)*
    - Xie, Sang Michael, et al., *[Data Selection for Language Models via Importance Resampling](https://proceedings.neurips.cc/paper_files/paper/2023/file/6b9aa8f418bde2840d5f4ab7a02f663b-Paper-Conference.pdf)*
    - Chen, Mayee, et al., *[Skill-it! A data-driven skills framework for understanding and training language models](https://proceedings.neurips.cc/paper_files/paper/2023/file/70b8505ac79e3e131756f793cd80eb8d-Paper-Conference.pdf)*


### Lecture 10: Principals in Prompt Engineering
- Pricipals in Prompt Engineering [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/PE.pdf)
- Chain of Thoughts [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/CoT.pdf)
- Reading: <br>
    - OpenAI, *[Prompt Engineering](https://platform.openai.com/docs/guides/prompt-engineering)*
    - Jason Wei, et. al. *[Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)*
    - Takeshi Kojima, et. al. *[Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2201.11903)*
    - Zhuosheng Zhang, et. al. *[Automatic Chain of Thought Prompting in Large Language Models](https://arxiv.org/abs/2210.03493)*
    - Shunyu Yao, et. al. *[Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601)* 

### Lecture 11: LLM Based Agents
- LLM Based Agents [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/Agents.pdf)
- Reading: <br>
    - Z. Xi et.al., *[The Rise and Potential of Large Language Model Based Agents: A Survey](https://arxiv.org/pdf/2309.07864)*
    - Andrew Ng, *[Agentic Reasoning](https://www.bilibili.com/video/BV1c1421U7yq/?spm_id_from=333.337.search-card.all.click&vd_source=2609112b8838130df3f5c7166ed6effb)*

### Guest Lecture II: <br> ###
- Building Brainiac Buddy with LLM Agents (Zihao Liu, Beijing International Center for Mathematical Research)

### Guest Lecture III: <br> ###
- Building LLM Agents with Alibaba MindOpt Studio  (Dr. Jianfeng Yang, Alibaba DAMO Academcy)
 
### Lecture 12: Retrieval Augmented Generation
- Slides are adapted from *[RAG from Scratch](https://www.youtube.com/watch?v=wd7TZ4w1mSw&list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x)*
- Reading: <br>
    - LangChain, *[RAG from Scratch](https://www.bilibili.com/video/BV1dm41127jc/?spm_id_from=333.788&vd_source=2609112b8838130df3f5c7166ed6effb)*
    - P. Lewis, et. al., *[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf)*
    - Y. Gao, et. al., *[Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997)*
 
### Lecture 13: Parameter-Efficient Fine-Tuning
- Low-Rank adaptation (LoRA); LoRA+; DoRA; LISA [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/LoRA.pdf)
- Reading: <br>
    - E. Hu, et. al., *[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)*
    - S. Hayou, et. al., *[LoRA+: Efficient Low Rank Adaptation of Large Models](https://arxiv.org/abs/2402.12354)*
    - S. Liu, et. al., *[DoRA: Weight-Decomposed Low-Rank Adaptation](https://arxiv.org/abs/2402.09353)*
    - R. Pan, et.al, *[LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning](https://arxiv.org/abs/2403.17919)*

### Lecture 14: Alignment
- SFT and RLHF [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/Alignment.pdf)
- Reading: <br>
    - Andrej Karpathy, *[State of GPT](https://www.bilibili.com/video/BV1ts4y1T7UH/?spm_id_from=333.337.search-card.all.click)* 
    - L. Ouyang, et. al., *[Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155)* 
    - Jan Leike, *[Aligning language models with humans](https://www.youtube.com/watch?v=DJ1Yy6Aquug&list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM&index=12&t=1858s)* 
    - Devon Wood-Thomas, *[AI Alignment and LLMs](https://www.cs.princeton.edu/courses/archive/fall22/cos597G/lectures/lec22.pdf)*
 
### Final Review
- Final review [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/Final_review.pdf)
