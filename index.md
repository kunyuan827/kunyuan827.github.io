---
layout: page
title: ""
---

<div style="display: flex; align-items: center; flex-wrap: wrap;">
  <img src="/images/kun_homepage.jpg" alt="kunyuan" style="width: 150px; height: auto; margin-right: 20px; margin-bottom: 10px;">
  <div>
    <strong>Kun Yuan (袁 坤)</strong><br>
    Assistant Professor, Center for Machine Learning Research, Peking University<br>
    Email: kunyuan AT pku.edu.cn<br>
    <a href="https://kunyuan827.github.io/resources/Kun_CV.pdf" target="_blank">CV</a> / <a href="https://scholar.google.com/citations?user=aMnHLz4AAAAJ&hl=en">Google scholar</a>
  </div>
</div>

I am an Assistant Professor at [Center for Machine Learning Research (CMLR)](https://cmlr.pku.edu.cn/) in Peking University. 

My research lies in the theoretical and algorithmic foundations in optimization, signal processing, machine learning, and data science. I currently focus on the development of fast, scalable, reliable, and distributed algorithms with applications in large-scale optimization, deep neural network training, federated learning, and Internet of things.  

<!-- I was the recipient of several academic awards, including the prestigious 2017 IEEE Signal Processing Society Young Author Best Paper Award, and the 2017 ICCM Distinguished Paper Award.  -->

Before joining Peking University, I was a staff algorithm engineer in the [Decision Intelligence Lab](https://damo.alibaba.com/labs/decision-intelligence) in Alibaba (US) Group led by Prof. [Wotao Yin](https://wotaoyin.mathopt.com/). I completed my Ph.D. in Electrical and Computer Engineering from [University of California, Los Angeles (UCLA)](https://www.ucla.edu/) in 2019 under the supervision of Prof. [Ali H. Sayed](https://asl.epfl.ch/biography/). I was a visiting researcher at [École Polytechnique Fédérale de Lausanne (EPFL)](https://www.epfl.ch/en/) from 2018 January to 2018 June, and a research intern at [Microsoft Research Redmond](https://www.microsoft.com/en-us/research/group/deep-learning-group/) from 2018 June to 2018 October.

I was the recipient of the 2025 IEEE CloudCom Distinguished Paper Award,  the 2017 IEEE Signal Processing Society Young Author Best Paper Award (joint with Dr. [Wei Shi](https://sites.google.com/view/wilburshi/home)), and the 2017 ICCM Distinguished Paper Award.

I currently serve as an Associate Editor for IEEE Transactions on Signal Processing and an Area Chair for both NeurIPS and ICML.

<mark> We are hiring PostDocs and Undergraduate Research Interns!!! </mark> <br> 

<mark> Drop me an email if you are interested in machine learning, optimization, and AI systems. </mark> <br> 

<!-- <mark> 正在招收2024年暑期科研实习生。项目简介及要求请参看该文档 [SummerIntern.pdf](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/resources/SummerIntern.pdf)。 </mark> <br> -->

<!-- <mark style="background-color: lightblue">我们课题组在2024 Spring还有一个轮转名额，欢迎叉院2023届博士生同学联系！</mark> <br> -->



### News
<!-- - [11/2022] We hosted *[2022 PKU Workshop on Operations Research and Machine Learning](http://conference.bicmr.pku.edu.cn/meeting/index?id=102)* online on Nov. 21 and Nov. 22. I gave a talk on *[DecentLaM: Decentralized Momentum SGD for Large-Batch Deep Training](https://arxiv.org/abs/2104.11981)*. Please check [Slides (on Github)](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/resources/DecentLaM.pdf) or [Slides (on Baidu Wangpan)](https://pan.baidu.com/s/1-p7JBdI7ctIZ1-4VbwAL-Q?pwd=bjb6). -->

<!-- - <mark> We're hiring 2024 Summer Research Interns. Please check this [document](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/resources/SummerIntern.pdf).</mark> <br> -->

<!-- - [06/2024] 正在招收2024年暑期实习生. 项目简介及要求请参看该 [文档](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/resources/SummerIntern.pdf).  -->

- [01/2026] I will teach an intensive course on [Optimization for Large Language Models](https://kunyuan827.github.io/llmopt2026/). I would like to express my sincere gratitude to the [Operations Research Society of China](https://www.orsc.org.cn/article/detail?id=1007) for the invitation and excellent organization.

- [01/2026] I will serve as an Associate Editor for IEEE Transactions on Signal Processing. 

- [01/2026] Very honored to give an one-hour talk at [Microsoft Research Asia ACE Talk](https://www.microsoft.com/en-us/research/academic-program/microsoft-research-asia-ace-talk/invited-speakers/) on [Memory-Efficient LLM Training via Implicit Structures](https://kunyuan827.github.io/resources/talk/MemoryEfficientLLM_MS.pdf).

- [12/2025] Our paper, *[A Mathematics-Inspired Learning-to-Optimize Framework for Decentralized Optimization](https://arxiv.org/abs/2410.01700)*,
 is accepted to IEEE Transactions on Signal Processing. Congratulations to all students and collaborators.

- [11/2025] The following papers are accepted to Journal of Machine Learning Research. Congratulations to all students and collaborators.
  * *[Decentralized Bilevel Optimization: A Perspective from Transient Iteration Complexity](https://arxiv.org/abs/2402.03167)*
  * *[Revisiting Gradient Normalization and Clipping for Nonconvex SGD under Heavy-Tailed Noise: Necessity, Sufficiency, and Acceleration](https://arxiv.org/abs/2410.16561)*
  * *[Optimal Complexity in Byzantine-Robust Distributed Stochastic Optimization with Data Heterogeneity](https://arxiv.org/abs/2503.16337)*

- [09/2025] Our new paper, *[CR-Net: Scaling Parameter-Efficient Training with Cross-Layer Low-Rank Structure](https://arxiv.org/abs/2509.18993)*, is online now. It introduces CR-Net, a cross-layer low-rank residual network for efficient LLM pre-training. By exploiting low-rank activation residuals, CR-Net outperforms state-of-the-art low-rank methods while cutting parameters, compute, and memory usage.

- [09/2025] Our new paper, *[Clapping: Removing Per-sample Storage for Pipeline Parallel Distributed Optimization with Communication Compression](https://arxiv.org/abs/2509.19029)*, is online now. It introduces Clapping, a communication-efficient algorithm for pipeline-parallel distributed optimization. By reusing data through lazy sampling, Clapping overcomes the sample-wise memory barrier, achieves convergence without unbiased gradient assumptions, and delivers strong accuracy–efficiency trade-offs across diverse learning tasks.

- [09/2025] The following papers are accepted to NeurIPS 2025. Congratulations to all collaborators and students.
  * *[MeCeFO: Enhancing LLM Training Robustness via Fault-Tolerant Optimization](https://arxiv.org/abs/2510.16415)*
  * *[Improving Model Representation and Reducing KV Cache via Skip Connections with First Value Heads](https://www.arxiv.org/abs/2510.16807)* 
  * *[MISA: Memory-Efficient LLMs Optimization with Module-wise Importance Sampling](https://openreview.net/forum?id=yISJGSdzdd)*

- [08/2025] We have a 90-min tutorial talk on our recent results for memory-efficient LLM pre-training. Slides are [here](./talks).

- [07/2025] Our new paper, *[Greedy Low-Rank Gradient Compression for Distributed Learning with Convergence Guarantees](https://arxiv.org/pdf/2507.08784)*, is now available on arXiv. In this work, we propose **GreedyLore**—the first greedy low-rank gradient compression algorithm for distributed learning with rigorous convergence guarantees.

- [06/2025] Our new paper, *[On the Linear Speedup of the Push-Pull Method for Decentralized Optimization over Digraphs](https://arxiv.org/pdf/2506.18075)*, is now available on arXiv. In this work, we establish for the first time that the Push-Pull method can achieve linear speedup in iteration complexity over directed graphs.

- [05/2025] The following papers are accepted to ICML 2025. Congratulations to all collaborators and students.
  * *[Efficient Multi-Objective Learning under Preference Guidance: A First-Order Penalty Approach](https://arxiv.org/abs/2504.02854)* <span style="color:red">[Spotlight]</span>
  * *[Achieving Linear Speedup and Optimal Complexity for Decentralized Optimization over Row-stochastic Networks](http://arxiv.org/abs/2506.04600)* <span style="color:red">[Spotlight]</span>
  * *[Subspace Optimization for Large Language Models with Convergence Guarantees](https://arxiv.org/pdf/2410.11289)*
  * *[A Memory Efficient Randomized Subspace Optimization Method for Training Large Language Models](https://arxiv.org/abs/2502.07222)*
  * *[Distributed Retraction-Free and Communication-Efficient Optimization on the Stiefel Manifold](https://arxiv.org/abs/2506.02879)*

- [04/2025] Our paper *[Understanding the Influence of Digraphs on Decentralized Optimization: Effective Metrics, Lower Bound, and Optimal Algorithm](https://arxiv.org/pdf/2312.04928.pdf)* is accepted by SIAM Journal on Optimization. 

- [02/2025] I will teach a course on *[Introduction to Foundation Models](./llm2025)* in Spring 2025.

- [02/2025] I will server as an Area Chair for [NeurIPS 2025](https://neurips.cc/).

- [02/2025] A new paper *[CE-LoRA: Computation-Efficient LoRA Fine-Tuning for Language Models](https://arxiv.org/pdf/2502.01378)* is now available on arXiv. In this work, we propose a computation-efficient LoRA variant to enhance computational efficiency while preserving memory efficiency.

- [02/2025] A new paper *[A Memory Efficient Randomized Subspace Optimization Method for Training Large Language Models](https://arxiv.org/pdf/2502.07222)* is now available on arXiv. In this work, we propose a Randomized Subspace Optimization framework for pre-training and fine-tuning LLMs with strong convergence guarantees.

- [01/2025] One paper is accepted to ICLR 2025. Congratulations to all collaborators!
  * *[Enhancing Zeroth-Order Fine-Tuning for Language Models with Low-Rank Structures](https://arxiv.org/pdf/2410.07698)* <br>

- [11/2024] I will server as an Area Chair for [ICML 2025](https://icml.cc/).

- [10/2024] A new paper *[Subspace Optimization for Large Language Models with Convergence Guarantees](https://arxiv.org/pdf/2410.11289)* is now available on arXiv. In this paper, we unexpectedly discover that GaLore does not always converge to the optimal solution and substantiate this finding with an explicit counterexample. We further propose a novel variant of GaLore that provably converges in stochastic optimization.

- [10/2024] A new paper *[Enhancing Zeroth-Order Fine-Tuning for Language Models with Low-Rank Structures](https://arxiv.org/pdf/2410.07698)* is now available on arXiv. In this work, we propose a low-rank zeroth-order gradient estimator and introduces a novel low-rank ZO algorithm to effectively fine-tune LLMs. It outperforms MeZO significantly.

- [10/2024] A new paper *[A Mathematics-Inspired Learning-to-Optimize Framework for Decentralized Optimization](https://arxiv.org/pdf/2410.01700)* is now available on arXiv. In this work, we present the first learning-to-optimize framework that surpasses state-of-the-art hand-crafted decentralized algorithms.

- [09/2024] One paper is accepted to NeurIPS 2024. Congratulations to my student Shuchen Zhu, Boao Kong, and all collaborators!
  * *[SPARKLE: A Unified Single-Loop Primal-Dual Framework for Decentralized Bilevel Optimization](https://openreview.net/pdf?id=g5DyqerUpX)* <br>

- [09/2024] I will be teaching a course on *[Optimization for Deep Learning](./dlopt2024)* in 2024 Fall. 

- [06/2024] I will give a 3-hour tutorial on *Efficient Optimization for Deep Learning* at *[Fudan University](https://www.fudan.edu.cn/)* on June 8th. Please check the *[slides](./talks)*.

- [05/2024] I will teach a short summer course titled *Efficient Optimization for Large Language Models* at *[Beijing Jiaotong University](https://www.bjtu.edu.cn/)* from July 7th to July 9th. It will be a condensed mix of my two regular classes *[Optimization for Deep Learning](./dlopt2023/)* and *[Large Language Models in Decision Intelligence](./llm2024)*. The syllabus is coming soon. 

- [05/2024] One paper is accepted to ICML 2024. Congratulations to my student Yutong He, Jie Hu, and all collaborators! 
  * *[Distributed Bilevel Optimization with Communication Compression](https://arxiv.org/pdf/2405.18858)* <br>
  
- [04/2024] My undergraduate students, [Ziheng Cheng](https://openreview.net/profile?id=~Ziheng_Cheng4) and Liyuan Liang, have been admitted to the UC Berkeley PhD Program. Additionally, Lujing Zhang has been admitted to the Carnegie Mellon University (CMU) PhD Program. Congratulations to all of them! We are currently hiring undergraduate research interns. We are committed to providing abundant resources and comprehensive instructions to support their involvement in cutting-edge research projects. <br>

- [04/2024] I will give a talk on [Asynchronous Diffusion Learning with Agent Subsampling and Local Updates](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/resources/talk/ICASSP2014.pdf) at [IEEE ICASSP 2024]([https://groups.oist.jp/mlss](https://2024.ieeeicassp.org/program-schedule/)).

- [03/2024] I will server as an Area Chair for [NeurIPS 2024](https://neurips.cc/).

- [03/2024] I will give a tutorial lecture on Distributed Machine Learning at [MLSS 2024](https://groups.oist.jp/mlss). Lecture slides can be found at [Distributed Machine Learning: Part I](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/resources/talk/DistributedML-PartI[Okinawa].pdf) and [Distributed Machine Learning: Part II](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/resources/talk/DistributedML-Part2[Okinawa].pdf).

- [02/2024] I will be teaching a course on *[Large Language Models in Decision Intelligence](./llm2024)* in 2024 Spring. 

- [02/2024] A new paper *[Decentralized Bilevel Optimization over Graphs: Loopless Algorithmic Update and Transient Iteration Complexity](https://arxiv.org/pdf/2402.03167.pdf)* is on arXiv now. We have calrified the joint influence of network topology and data heterogeneity on decentralized bilevel optimization.
  
- [01/2024] One paper is accepted to ICLR 2024. Congratulations to my student Ziheng Cheng and all collaborators! 
  * *[Momentum Benefits Non-IID Federated Learning Simply and Provably](https://arxiv.org/pdf/2306.16504.pdf)* <br>  

- [12/2023] A new paper *[Towards Better Understanding the Influence of Directed Networks on Decentralized Stochastic Optimization](https://arxiv.org/pdf/2312.04928.pdf)* is on arXiv now. Surprisingly, we find that spectral gap is not enough to capture the influence of directed networks and the equilibrium skewness matters a lot! We also establish the lower bound for decentralized algorithms with clomun-stochastic mixing matrices. 

- [11/2023] We will organize a session on [Decentralized Optimization and Learning](https://css.paperplaza.net/conferences/conferences/CDC23/program/CDC23_ContentListWeb_1.html#wea05) in IEEE CDC 2023. 

- [11/2023] One paper is accepted by Signal Processing.
  * *[An Enhanced Gradient-Tracking Bound for Distributed Online Stochastic Convex Optimization](https://arxiv.org/abs/2301.02855)*

- [09/2023] A new paper *[Sharper Convergence Guarantees for Federated Learning with Partial Model Personalization](https://arxiv.org/abs/2309.17409)* is on arXiv now. We establish new state-of-the-art convergence rates for federated learning with partial model personalization!

- [09/2023] One paper is accepted to NeurIPS 2023.
  * *[Unbiased Compression Saves Communication in Distributed Optimization: When and How Much?](https://arxiv.org/abs/2305.16297)* <br>
  
  Congratulations to my student Yutong He on publishing his first paper! 

- [09/2023] I will be teaching a course on *[Optimization for Deep Learning](./dlopt2023/)* in 2023 Fall. 

- [09/2023] One paper is accepted by Journal of Machine Learning Research (JMLR).
  * *[Removing Data Heterogeneity Influence Enhances Network Topology Dependence of Decentralized SGD](https://arxiv.org/pdf/2105.08023.pdf)*


- [07/2023] Two papers are accepted to IEEE CDC 2023.
  * *[Achieving Linear Speedup with Network-Independent Learning Rates in Decentralized Stochastic Optimization](https://ieeexplore.ieee.org/abstract/document/10384058)*
  * *[On the Performance of Gradient Tracking with Local Updates](https://arxiv.org/abs/2210.04757)*
    
  Congratulations to my student Hao Yuan and my collaborator Edward Nguyen on publishing their first papers! 

- [06/2023] A new paper *[Momentum Benefits Non-IID Federated Learning Simply and Provably](https://arxiv.org/pdf/2306.16504.pdf)* is on arXiv now. An interesting message is that FedAvg can converge without data heterogeneity assumption when incorporating momentum!

- [05/2023] A new paper *[Unbiased Compression Saves Communication in Distributed Optimization: When and How Much?](https://arxiv.org/pdf/2305.16297.pdf)* is on arXiv now. 

- [05/2023] A new paper *[Lower Bounds and Accelerated Algorithms in Distributed Stochastic Optimization with Communication Compression](https://arxiv.org/abs/2305.07612)* is on arXiv now. Please also check [Slides (on Github)](/resources/KunYuan_OptimalCompression.pdf) or [Slides (on Baidu Wangpan)](https://pan.baidu.com/s/1q5ROxiIdkqTCiXkNJmJgqg?pwd=atfh) for this paper. Some preliminary results of this paper have been published in NeurIPS 2022, check [this paper](https://arxiv.org/pdf/2206.03665.pdf). 

- [04/2023] Two papers are accepted to ICML 2023. 

  * *[DSGD-CECA: Decentralized SGD with Communication-Optimal Exact Consensus Algorithm](https://arxiv.org/pdf/2306.00256.pdf)*
  * *[AdaNPC: Exploring Non-Parametric Classifier for Test-Time Adaptation](https://arxiv.org/pdf/2304.12566.pdf)*
 
- [02/2023] One paper *[BEVHeight: A Robust Framework for Vision-based Roadside 3D Object Detection](https://arxiv.org/abs/2303.08498)* is accepted to CVPR 2023.

- [01/2023] A new paper *[An Enhanced Gradient-Tracking Bound for Distributed Online Stochastic Convex Optimization](https://arxiv.org/abs/2301.02855)* is on arXiv now. We establish enhanced rates for Gradient Tracking methods under the online stochastic convex settings.

- [11/2022] I gave a talk in *[BICMR](https://bicmr.pku.edu.cn/)* on *Accelerating Decentralized SGD with Sparse and Effecitve Topologies*, which includes our rescent results on *[Exponential Graphs](https://arxiv.org/abs/2110.13363)*, *[EquiToPo Graphs](https://arxiv.org/abs/2210.07881)*, and *[BlueFog](https://arxiv.org/abs/2111.04287)*. Please check [Slides (on Github)](/resources/Topologies_for_decentralized_deep_learning.pdf) or [Slides (on Baidu Wangpan)](https://pan.baidu.com/s/1UKEEPiISeNfxySg8-DJe2w?pwd=8849).

- [11/2022] We hosted *[2022 PKU Workshop on Operations Research and Machine Learning](http://conference.bicmr.pku.edu.cn/meeting/index?id=102)* online on Nov. 21 and Nov. 22. I gave a talk on *[DecentLaM: Decentralized Momentum SGD for Large-Batch Deep Training](https://arxiv.org/abs/2104.11981)*. Please check [Slides (on Github)](/resources/DecentLaM.pdf) or [Slides (on Baidu Wangpan)](https://pan.baidu.com/s/1-p7JBdI7ctIZ1-4VbwAL-Q?pwd=bjb6).

<!-- - [10/2022] Our paper *[Optimal Complexity in Non-Convex Decentralized Learning over Time-Varying Networks](https://arxiv.org/abs/2211.00533)* was accepted to NeurIPS 2022 Workshop on [Optimization for Machine Learning](https://opt-ml.org/index.html). -->

- [09/2022] Three papers are accepted to NeurIPS 2022. 

  * [*Revisiting Optimal Convergence Rate for Smooth and Non-convex Stochastic Decentralized Optimization*](https://arxiv.org/pdf/2210.07863.pdf)
  * [*Communication-Efficient Topologies for Decentralized Learning with O(1) Consensus Rate*](https://arxiv.org/pdf/2210.07881.pdf)
  * [*Lower Bounds and Nearly Optimal Algorithms in Distributed Learning with Communication Compression*](https://arxiv.org/pdf/2206.03665.pdf)
  

- [09/2022] Prof. Wotao Yin was invited to give a keynote talk on our recent work *[Lower Bounds and Nearly Optimal Algorithms in Distributed Learning with Communication Compression](https://arxiv.org/abs/2206.03665)* in the [CrossFL 2022](https://crossfl2022.github.io/) workshop. Please check the [slides](https://crossfl2022.github.io/presentations/OptimalCompression.pdf) and the [Youtube video](https://youtu.be/0mV85uGMpXA?t=19516).  

<!-- I received my M. E. degree from [University of Science and Technology of China (USTC)](https://en.ustc.edu.cn/) in 2014 supervised by Prof. [Qing Ling](https://scholar.google.com/citations?user=u70vRDYAAAAJ&hl=en), and B. E. degree from [Xidian University](https://en.xidian.edu.cn/) in 2011.  -->

<!-- ### Academic Awards

- 2017 IEEE Signal Processing Society Young Author Best Paper Award [[Awardee list]](https://signalprocessingsociety.org/sites/default/files/uploads/get_involved/awards/Young_Author_Best_Paper.pdf) [[News]](https://sist.ustc.edu.cn/2018/0423/c5146a257808/page.htm) <br>

- 2017 ICCM Distinguished Paper Award -->

