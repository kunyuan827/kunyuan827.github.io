---
title: ""
permalink: "/llm2025/"
layout: page
---

## PKU Class 2025 Spring: Introduction to Foundation Models

Instructor: **Kun Yuan** (kunyuan@pku.edu.cn) <br>

Teaching assistants: 
- **Jie Hu** (hujie@stu.pku.edu.cn) <br>
- **Yipeng Hu** (2301213082@stu.pku.edu.cn) <br>
- **Qiulin Shang** (2100013145@stu.pku.edu.cn) <br>
- **Yilong Song** (2301213059@pku.edu.cn) <br>

Office hour: 4pm - 5pm Wednesday, 静园六院220

## References
Stanford CS224n: Natural Language Processing with Deep Learning

## Lectrures

### Lecture 1: Introduction to LLM <br>
- Introduction to large language model [[Slides1]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM2025/Intro1_llm.pdf) [[Slides2]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/DLOpt2024/Intro2.pdf)
- Reading: <br>
    - Andrej Karpathy, *[State of GPT](https://www.bilibili.com/video/BV1ts4y1T7UH/?spm_id_from=333.337.search-card.all.click)* <br>
    - Andrej Karpathy, *[The busy person's intro to LLM](https://www.bilibili.com/video/BV1NH4y1m78m/?spm_id_from=333.337.search-card.all.click&vd_source=2609112b8838130df3f5c7166ed6effb)* <br>

### Lecture 2: Basics in machine learning <br>
- Warm up: Preliminary [[Notes]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM2025/notes_0.pdf) <br>
- Linear regression; Logistic regression; Multi-classification; Neural network [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM2025/02_Regression.pdf)
- Reading: <br>
    - Stanford CS231n, *[Linear classification](https://cs231n.github.io/linear-classify/)* <br>
    - Stanford CS231n, *[Neural netowrk part I](https://cs231n.github.io/neural-networks-1/)* 

### Lecture 3: Gradient descent <br>
- Convex set; Convex functions; Convex problems; Gradient descent [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM2025/03_GD.pdf) [[Notes]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM2025/notes_1.pdf)
- Forward-backward propagation [[Notes]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM2025/notes_2.pdf)

### Lecture 4: Stochastic gradient descent <br>
- Stochastic gradient descent (SGD); mini-batch SGD [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM2025/04_SGD.pdf) [[Notes]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM2025/notes_3.pdf) <br>
- Mini-batch forward-backward propagation [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM2025/05_SGD_FB.pdf) 
- Reading: <br>
    - Leon Bottou, *[Large-Scale Machine Learning with Stochastic Gradient Descent](https://datajobs.com/data-science-repo/Stochastic-Gradient-Descent-[Leon-Bottou].pdf)* <br>
    - Yingjie Tian et. al., *[Recent Advances in Stochastic Gradient Descent in Deep Learning](https://www.mdpi.com/2227-7390/11/3/682)* <br>

### Lecture 5: Adavanced optimizers <br>
- Momentum SGD; Nesterov SGD [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM2025/06_ACC_SGD.pdf) <br>
- Adaptive SGD; AdaGrad; RMSProp; Adam [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM2025/07_Adaptive_SGD.pdf) <br>

### Lecture 6: Language models <br>
- Word embedding; Language models; Recurrent neural networks [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/DLOpt2024/03_langmodel.pdf) <br>
- Reading: <br>
    - *[Stanford CS224N: Week 3](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1246/)* <br>

### Lecture 7: Transformers <br> 
- Seq2seq models; cross-attention; self-attention; transformers [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/DLOpt2024/04_transformer.pdf) <br>
- Reading: <br>
    - *[Stanford CS224N: Week 4](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1246/)* <br>
    - Ilya Sutskever et. al., *[Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215)* <br>
    - Ashish Vaswani et. al., *[Attention Is All You Need](https://arxiv.org/pdf/1706.03762)* <br>

### Lecture 8: Flash Attention <br>
- Memory access cost; Kernal fusion; Flash attention [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/DLOpt2024/22_FlashAttention.pdf)

### Lecture 9: Midterm Review <br>
- Good Luck [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM2025/10_Midterm_review.pdf)

### Lecture 10: Mixed-Precision <br>
- FP32; FP16; Mixed-precision training [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/DLOpt2024/13_mixed.pdf)

### Lecture 11: 3D Parallelism <br>
- Data Parallelism; Pipeline Parallelism; Tensor Parallelism [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/DLOpt2024/19_Parallesm.pdf)
