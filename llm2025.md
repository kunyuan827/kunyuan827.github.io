---
title: ""
permalink: "/llm2025/"
layout: page
---

## PKU Class 2025 Spring: Introduction to Foundation Models

Instructor: **Kun Yuan** (kunyuan@pku.edu.cn) <br>

Teaching assistants: 
- **Jie Hu** (hujie@stu.pku.edu.cn) <br>
- **Yipeng Hu** (2301213082@stu.pku.edu.cn) <br>
- **Qiulin Shang** (2100013145@stu.pku.edu.cn) <br>
- **Yilong Song** (2301213059@pku.edu.cn) <br>

Office hour: 4pm - 5pm Wednesday, 静园六院220

## References
Stanford CS224n: Natural Language Processing with Deep Learning

## Lectrures

### Lecture 1: Introduction to LLM <br>
- Introduction to large language model [[Slides1]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM2025/Intro1_llm.pdf) [[Slides2]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/DLOpt2024/Intro2.pdf)
- Reading: <br>
    - Andrej Karpathy, *[State of GPT](https://www.bilibili.com/video/BV1ts4y1T7UH/?spm_id_from=333.337.search-card.all.click)* <br>
    - Andrej Karpathy, *[The busy person's intro to LLM](https://www.bilibili.com/video/BV1NH4y1m78m/?spm_id_from=333.337.search-card.all.click&vd_source=2609112b8838130df3f5c7166ed6effb)* <br>

### Lecture 2: Basics in machine learning <br>
- Warm up: Preliminary [[Notes]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM2025/notes_0.pdf) <br>
- Linear regression; Logistic regression; Multi-classification; Neural network [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM2025/02_Regression.pdf)
- Reading: <br>
    - Stanford CS231n, *[Linear classification](https://cs231n.github.io/linear-classify/)* <br>
    - Stanford CS231n, *[Neural netowrk part I](https://cs231n.github.io/neural-networks-1/)* 

### Lecture 3: Gradient descent <br>
- Convex set; Convex functions; Convex problems; Gradient descent [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM2025/03_GD.pdf) [[Notes]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM2025/notes_1.pdf)
- Forward-backward propagation [[Notes]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM2025/notes_2.pdf)

### Lecture 4: Stochastic gradient descent <br>
- Stochastic gradient descent (SGD); mini-batch SGD [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM2025/04_SGD.pdf) [[Notes]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM2025/notes_3.pdf) <br>
- Mini-batch forward-backward propagation [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM2025/05_SGD_FB.pdf) 
- Reading: <br>
    - Leon Bottou, *[Large-Scale Machine Learning with Stochastic Gradient Descent](https://datajobs.com/data-science-repo/Stochastic-Gradient-Descent-[Leon-Bottou].pdf)* <br>
    - Yingjie Tian et. al., *[Recent Advances in Stochastic Gradient Descent in Deep Learning](https://www.mdpi.com/2227-7390/11/3/682)* <br>

### Lecture 5: Adavanced optimizers <br>
- Momentum SGD; Nesterov SGD [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM2025/06_ACC_SGD.pdf) <br>
- Adaptive SGD; AdaGrad; RMSProp; Adam [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM2025/07_Adaptive_SGD.pdf) <br>

### Lecture 6: Language models <br>
- Word embedding; Language models; Recurrent neural networks [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/DLOpt2024/03_langmodel.pdf) <br>
- Reading: <br>
    - *[Stanford CS224N: Week 3](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1246/)* <br>

### Lecture 7: Transformers <br> 
- Seq2seq models; cross-attention; self-attention; transformers [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/DLOpt2024/04_transformer.pdf) <br>
- Reading: <br>
    - *[Stanford CS224N: Week 4](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1246/)* <br>
    - Ilya Sutskever et. al., *[Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215)* <br>
    - Ashish Vaswani et. al., *[Attention Is All You Need](https://arxiv.org/pdf/1706.03762)* <br>

### Lecture 8: Flash Attention <br>
- Memory access cost; Kernal fusion; Flash attention [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/DLOpt2024/22_FlashAttention.pdf)

### Lecture 9: Midterm Review <br>
- Good Luck [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM2025/10_Midterm_review.pdf)

### Lecture 10: Mixed-Precision <br>
- FP32; FP16; Mixed-precision training [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/DLOpt2024/13_mixed.pdf)

### Lecture 11: 3D Parallelism <br>
- Data Parallelism; Pipeline Parallelism; Tensor Parallelism [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/DLOpt2024/19_Parallesm.pdf)
 
### Lecture 12: Popular LLM Models <br>
- Teacher forcing; Pretrain and Finetuning; BERT; GPTs [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/DLOpt2024/11_Bert_and_GPT.pdf)
- DeepSeek [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/DLOpt2024/19_Parallesm.pdf)
- Reading: <br>
    - J. Devlin et.al., *[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)*
    - A. Radford et.al., *[Improving Language Understanding by Generative Pre-Training](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)*
    - A. Radford et.al., *[Language models are unsupervised multitask learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)*
    - T. B. Brown et.al., *[Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)*
    - DeepSeek, *[DeepSeek-V3 Technical Report](https://arxiv.org/pdf/2412.19437)* 

### Lecture 13: Scaling Law <br>
- Scaling law [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/Scaling_law.pdf)
- Reading: <br>
    - Jared Kaplan, et. al. *[Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)*
    - Jordan Hoffmann, et. al. *[Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)*

### Lecture 14: Principals in Prompt Engineering
- Pricipals in Prompt Engineering [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM2025/PE.pdf)
- Chain of Thoughts [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/CoT.pdf)
- Reading: <br>
    - OpenAI, *[Prompt Engineering](https://platform.openai.com/docs/guides/prompt-engineering)*
    - Jason Wei, et. al. *[Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)*
    - Takeshi Kojima, et. al. *[Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2201.11903)*
    - Zhuosheng Zhang, et. al. *[Automatic Chain of Thought Prompting in Large Language Models](https://arxiv.org/abs/2210.03493)*
    - Shunyu Yao, et. al. *[Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601)*
 
### Lecture 15: Data Preparation
- Data source; Deduplication; Quality filtering; Sensitive information reduction; Data composition; Data curriculum [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/DataPrep.pdf)
- Reading: <br>
    - Penedo, Guilherme, et al., *[The Refined Web Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only](https://arxiv.org/pdf/2306.01116)*
    - Soldaini, Luca, et al., *[Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research](https://arxiv.org/pdf/2402.00159)*
    - Kandpal, Nikhil, Eric Wallace, and Colin Raffel., *[Deduplicating Training Data Mitigates Privacy Risks in Language Models](https://proceedings.mlr.press/v162/kandpal22a/kandpal22a.pdf)*
    - Xie, Sang Michael, et al., *[Data Selection for Language Models via Importance Resampling](https://proceedings.neurips.cc/paper_files/paper/2023/file/6b9aa8f418bde2840d5f4ab7a02f663b-Paper-Conference.pdf)*
    - Chen, Mayee, et al., *[Skill-it! A data-driven skills framework for understanding and training language models](https://proceedings.neurips.cc/paper_files/paper/2023/file/70b8505ac79e3e131756f793cd80eb8d-Paper-Conference.pdf)*
 
### Lecture 16: LLM Based Agents
- LLM Based Agents [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/Agents.pdf)
- Reading: <br>
    - Z. Xi et.al., *[The Rise and Potential of Large Language Model Based Agents: A Survey](https://arxiv.org/pdf/2309.07864)*
    - Andrew Ng, *[Agentic Reasoning](https://www.bilibili.com/video/BV1c1421U7yq/?spm_id_from=333.337.search-card.all.click&vd_source=2609112b8838130df3f5c7166ed6effb)*

### Lecture 17: Parameter-Efficient Fine-Tuning
- Low-Rank adaptation (LoRA); LoRA+; DoRA; LISA; BAdam [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM2025/fine_tune.pdf)
- Reading: <br>
    - E. Hu, et. al., *[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)*
    - S. Hayou, et. al., *[LoRA+: Efficient Low Rank Adaptation of Large Models](https://arxiv.org/abs/2402.12354)*
    - S. Liu, et. al., *[DoRA: Weight-Decomposed Low-Rank Adaptation](https://arxiv.org/abs/2402.09353)*
    - R. Pan, et.al., *[LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning](https://arxiv.org/abs/2403.17919)*
    - Q. Luo, et.al., *[BAdam: A memory efficient full parameter optimization method for large language models](https://proceedings.neurips.cc/paper_files/paper/2024/hash/2c570b0f9938c7a58a612e5b00af9cc0-Abstract-Conference.html)*
 
### Lecture 18: Inference
- KV Cache; MLA; H2O; Streaming LLM; Quest; Speculative decoding; Page Attention [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM2025/24_infer_lecture.pdf)
- Reading: <br>
    - J. Ainslie et al., *[GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://arxiv.org/abs/2305.13245)*
    - DeepSeek Team, *[DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434)*
    - Z. Zhang et al., *[H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models](https://arxiv.org/abs/2306.14048)*
    - G. Xiao et al., *[Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/abs/2309.17453)*
    - J. Tang et al., *[Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference](https://arxiv.org/abs/2406.10774)*
    - M. Stern et al., *[Blockwise Parallel Decoding for Deep Autoregressive Models](https://arxiv.org/abs/1811.03115)*
    - W. Kwon et al., *[Efficient Memory Management for Large Language Model Serving with Paged Attention](https://arxiv.org/abs/2309.06180)*

### Lecture 19: Alignment
- SFT and RLHF [[Slides]](https://github.com/kunyuan827/kunyuan827.github.io/raw/master/teaching/LLM/Alignment.pdf)
- Proximal policy optimization (PPO) (Slides are adapted from Hung-yi Lee's lecture at [Bilibili](https://www.bilibili.com/video/BV1Az1kYjEgV?spm_id_from=333.788.videopod.episodes&vd_source=2609112b8838130df3f5c7166ed6effb&p=5))
- Reasoning Large Language Models; DeepSeek-R1  (Slides are adapted from Hung-yi Lee's lecture at [Bilibili](https://www.bilibili.com/video/BV1yyLdz9EVe/?spm_id_from=333.337.search-card.all.click))
- Reading: <br>
    - Andrej Karpathy, *[State of GPT](https://www.bilibili.com/video/BV1ts4y1T7UH/?spm_id_from=333.337.search-card.all.click)* 
    - L. Ouyang, et. al., *[Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155)* 
    - Jan Leike, *[Aligning language models with humans](https://www.youtube.com/watch?v=DJ1Yy6Aquug&list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM&index=12&t=1858s)* 
    - Devon Wood-Thomas, *[AI Alignment and LLMs](https://www.cs.princeton.edu/courses/archive/fall22/cos597G/lectures/lec22.pdf)*
    - DeepSeek Team, *[DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948)*
